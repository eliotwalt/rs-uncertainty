{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UQ metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import rasterio, yaml, os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import *\n",
    "\n",
    "PREDICTIONS_DIR = Path(\"results/dev/2023-03-14_15-45-23\")\n",
    "PKL_DIR = Path('data/pkl/2021-05-18_10-57-45')\n",
    "GT_DIR = Path('data/preprocessed')\n",
    "NUM_VARS = 5\n",
    "VARIABLES = ['P95', 'MeanH', 'Dens', 'Gini', 'Cover']\n",
    "\n",
    "EAST = ['346', '9', '341', '354', '415', '418', '416', '429', '439', '560', '472', '521', '498',\n",
    "        '522', '564', '764', '781', '825', '796', '805', '827', '891', '835', '920', '959', '1023', '998',\n",
    "        '527', '477', '542', '471']\n",
    "WEST = ['528', '537', '792', '988', '769']\n",
    "NORTH = ['819', '909', '896']\n",
    "ALL = EAST + WEST + NORTH\n",
    "\n",
    "with (PKL_DIR / 'stats.yaml').open() as fh:\n",
    "    # load training set statistics for data normalization\n",
    "    stats = yaml.safe_load(fh)\n",
    "    labels_mean = np.array(stats['labels_mean'])\n",
    "\n",
    "projects = [f.stem.split(\"_\")[0] for f in PREDICTIONS_DIR.glob('*_mean.tif') if f.stem.split(\"_\")[0] in ALL]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative metrics\n",
    "\n",
    "Let $\\mathcal{D}=\\left\\{(\\mathbf{x}_i, \\mathbf{y}_i) \\in \\mathcal{X}\\times\\mathcal{Y}\\right\\}_{i=1,\\ldots,N}$ be the test set and $\\mathcal{P}=\\left\\{(\\hat\\mu_ i, \\hat\\sigma_ i^2)  \\in \\mathcal{X}\\times\\mathcal{Y}\\right\\}_{i=1,\\ldots,N}$ be the corresponding pixel-wise predicted mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCE(variance, mean, gt, n_bins):\n",
    "    \"\"\"\n",
    "    Compute UCE as defined by Laves et al., Recalibration of Aleatoric and Epistemic Regression Uncertainty in Medical Imaging,\n",
    "    arXiv:2104.12376v1, 2021\n",
    "\n",
    "    Args:\n",
    "    - variance (np.ndarray[d, n]): predicted variance for each variable (d) and each pixel in the dataset (n)\n",
    "    - mean (np.ndarray[d, n]): predicted mean for each variable (d) and each pixel in the dataset (n)\n",
    "    - gt (np.ndarray[d, n]): ground truth for each variable (d) and each pixel in the dataset (n)\n",
    "    - n_bins (int): number of bins\n",
    "\n",
    "    Returns:\n",
    "    - uce (np.ndarray[d]): UCE of each variables\n",
    "    - mean_mses (np.ndarray[n_bind]): mean MSE in each bins\n",
    "    - mean_vars (np.ndarray[n_bind]): mean variance in each bins\n",
    "    - prop_in_bins (np.ndarray[n_bind]): proportion of the dataset in each bins\n",
    "    \"\"\"\n",
    "    d = gt.shape[0]\n",
    "    # Compute UCE for each variables\n",
    "    uce = np.empty((d,))\n",
    "    prop_in_bins = np.empty((d, n_bins))\n",
    "    mean_mses = np.empty((d, n_bins))\n",
    "    mean_vars = np.empty((d, n_bins))\n",
    "    for i, (var, mu, tgt) in enumerate(zip(variance, mean, gt)):\n",
    "        # Linear binning\n",
    "        bins = np.linspace(var.min(), var.max(), n_bins)\n",
    "        # Get variance bin indexes\n",
    "        bins_ids = np.digitize(var, bins=bins)\n",
    "        # Loop on bins to compute statistics\n",
    "        _uce = 0\n",
    "        for bin_id in np.unique(bins_ids)-1:\n",
    "            # Select bin\n",
    "            pos = bins_ids==bin_id+1\n",
    "            prop_in_bin = pos.astype(\"float\").mean() # bin_size / N\n",
    "            bin_var = var[pos]\n",
    "            bin_mean = mu[pos]\n",
    "            bin_tgt = tgt[pos]\n",
    "            # Compute stats\n",
    "            mean_var = bin_var.mean()\n",
    "            mean_mse = ((bin_mean-bin_tgt)**2).mean()\n",
    "            _uce += prop_in_bin * np.abs(mean_var- mean_mse)\n",
    "            # keep result\n",
    "            prop_in_bins[i,bin_id] = prop_in_bin\n",
    "            mean_mses[i,bin_id] = mean_mse \n",
    "            mean_vars[i,bin_id] = mean_var\n",
    "        uce[i] = _uce\n",
    "    return uce, mean_mses, mean_vars, prop_in_bins\n",
    "\n",
    "def ENCE(variance, mean, gt, n_bins):\n",
    "    \"\"\"\n",
    "    Compute ENCE as defined by Levi et al., Evaluating and Calibrating Uncertainty Prediction in Regression Tasks,\n",
    "    arXiv:1905.11659v3, 2023\n",
    "\n",
    "    Args:\n",
    "    - variance (np.ndarray[d, n]): predicted variance for each variable (d) and each pixel in the dataset (n)\n",
    "    - mean (np.ndarray[d, n]): predicted mean for each variable (d) and each pixel in the dataset (n)\n",
    "    - gt (np.ndarray[d, n]): ground truth for each variable (d) and each pixel in the dataset (n)\n",
    "    - n_bins (int): number of bins\n",
    "\n",
    "    Returns:\n",
    "    - ence (np.ndarray[d]): UCE of each variables\n",
    "    - bins_mean_rmse (np.ndarray[n_bind]): averager mse in each bins\n",
    "    - bins_mean_std (np.ndarray[n_bind]): average std in each bins\n",
    "    - bins_proportions (np.ndarray[n_bind]): proportion of the dataset in each bins\n",
    "    \"\"\"\n",
    "    _, mean_mses, mean_vars, bins_proportions = UCE(variance, mean, gt, n_bins)\n",
    "    mean_rmses, mean_stds = np.sqrt(mean_mses), np.sqrt(mean_vars)\n",
    "    ence = (np.abs(mean_stds-mean_rmses) / mean_stds).mean(1)\n",
    "    return ence, mean_rmses, mean_stds, bins_proportions\n",
    "\n",
    "def UCE_p(variance, mean, gt, n_bins):\n",
    "    uce, mean_mses, mean_vars, prop_in_bins = UCE(variance, mean, gt, n_bins)\n",
    "    deltas = np.abs(mean_mses-mean_vars).max(1)\n",
    "    uce_p = uce / deltas\n",
    "    return uce_p, mean_mses, mean_vars, prop_in_bins\n",
    "\n",
    "def ENCE_p(variance, mean, gt, n_bins):\n",
    "    M = n_bins\n",
    "    N = variance.shape[1]\n",
    "    ence, mean_rmses, mean_stds, bins_proportions = ENCE(variance, mean, gt, n_bins)\n",
    "    phis = (np.abs(mean_rmses-mean_stds)/mean_stds).max(1)\n",
    "    uce_p = N/(M*phis)*ence\n",
    "    return uce_p, mean_rmses, mean_stds, bins_proportions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Uncertainty Calibration Error (UCE) [Laves 2020, Laves 2021, Levi 2019, Becker 2023] and Expected Normalized Calibration Error (ENCE) [Levi 2019, Zhou 2021a]\n",
    "\n",
    "Those metrics measure how close are the mean variance (or std) and the empirical MSE (or RMSE). As these two quantities are equal, they should be minimized $\\text{UCE}(\\downarrow)$ and $\\text{ENCE}(\\downarrow)$. \n",
    "\n",
    "Let $B_k$, $k=1,\\ldots,M$ indicate the set of data index falling the the $k$-th bin of variance, i.e \n",
    "$$\n",
    "B_k=\\left\\{\n",
    "    i\\in\\{1,\\ldots,N\\}: \\sigma_i^2 \\in \\left[\n",
    "        \\frac{\\sigma_{max}^2-\\sigma_{min}^2}{M}k, \\frac{\\sigma_{max}^2-\\sigma_{min}^2}{M}(k+1)\n",
    "    \\right[\n",
    "\\right\\}.\n",
    "$$\n",
    "We define the empirical variance in bin $k$, $\\bar\\delta_k^2$, as the empirical MSE in the bin and the mean variance in bin $k$, $\\bar\\sigma_k^2$, as the average predicted variance:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\bar\\delta_k^2 &= \\frac{1}{|B_k|}\\sum_{i\\in B_k}(\\hat\\mu_i-\\mathbf{y}_i)^2 \\\\\n",
    "    \\bar\\sigma_k^2 &= \\frac{1}{|B_k|}\\sum_{i\\in B_k}\\sigma_i^2\n",
    "\\end{align}\n",
    "$$\n",
    "Then, [Laves 2021] and [Levi 2019] define UCE and ENCE, respectively, as:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{UCE} &= \\sum_{k=1}^{M}\\frac{|B_k|}{N}|\\bar\\sigma_k^2-\\bar\\delta_k^2| \\\\\n",
    "    \\text{ENCE} &=  \\frac{1}{N}\\sum_{k=1}^{M}\\frac{|\\bar\\sigma_k-\\bar\\delta_k|}{\\bar\\sigma_k} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized UCE ($\\text{UCE\\%}$)\n",
    "Let $\\Delta_k=|\\bar\\sigma_k^2-\\bar\\delta_k^2|$ and $\\Delta_{min}$, $\\Delta_{max}$ be the minimum and maximum values taken by that variable across bins. Then we have\n",
    "$$\n",
    "\\begin{align}\n",
    "         0 &\\leq \\Delta_{min} &\\leq \\text{UCE} &\\leq \\Delta_{max} \\\\\n",
    "   \\iff  0  &\\leq \\frac{\\Delta_{min}}{\\Delta_{max}} &\\leq \\frac{\\text{UCE}}{\\Delta_{max}} &\\leq 1\n",
    "\\end{align}\n",
    "$$\n",
    "Considering that, we propose to report $\\text{UCE\\%}=\\frac{1}{\\Delta_{max}}\\text{UCE}$, a normalized version of $\\text{UCE}$ that is bounded in $[0,1]$ for easier comparison across variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized ENCE ($\\text{ENCE\\%}$)\n",
    "Let $\\Phi_k = \\frac{|\\bar\\sigma_k-\\bar\\delta_k|}{\\bar\\sigma_k}$ and $\\Phi_{min}$, $\\Phi_{max}$ be the minimum and maximum values taken by that variable across bins. Then we have,\n",
    "$$\n",
    "\\begin{align}\n",
    "            0 &\\leq \\frac{M}{N}\\Phi_{min} &\\leq \\text{ENCE} &\\leq \\frac{M}{N}\\Phi_{max} \\\\\n",
    "      \\iff  0 &\\leq \\frac{\\Phi_{min}}{\\Phi_{max}} &\\leq \\frac{N}{M}\\frac{\\text{ENCE}}{\\Phi_{max}} &\\leq 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Considering that, we propose to report $\\text{ENCE\\%}=\\frac{N}{M\\Phi_{max}}\\text{ENCE}$, a normalized version of $\\text{ENCE}$ that is bounded in $[0,1]$ for easier comparison across variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P95: UCE=3.15441, UCE_p=0.01153, ENCE=0.30003, ENCE_p=45352.27242\n",
      "MeanH: UCE=1.52845, UCE_p=0.01255, ENCE=0.40573, ENCE_p=76227.82657\n",
      "Dens: UCE=3020.13470, UCE_p=0.36394, ENCE=306.36776, ENCE_p=58437.30030\n",
      "Gini: UCE=0.00113, UCE_p=0.03220, ENCE=0.22589, ENCE_p=43797.83451\n",
      "Cover: UCE=6519.76348, UCE_p=0.65878, ENCE=315.67318, ENCE_p=44061.70232\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "means, variances, gts = [], [], []\n",
    "for project in WEST:\n",
    "    try:\n",
    "        mean_file = os.path.join(PREDICTIONS_DIR, f\"{project}_mean.tif\")\n",
    "        with rasterio.open(mean_file) as fh:\n",
    "            mean = fh.read(fh.indexes)\n",
    "        with rasterio.open(PREDICTIONS_DIR / (project + '_variance.tif')) as fh:\n",
    "            variance = fh.read(fh.indexes)\n",
    "        with rasterio.open(GT_DIR / (project + '.tif')) as fh:\n",
    "            gt = fh.read(fh.indexes)\n",
    "            gt_mask = fh.read_masks(1).astype(bool)\n",
    "    except:\n",
    "        continue\n",
    "    mask = ~np.isnan(mean).all(0)\n",
    "    means.append(mean[:,mask]) \n",
    "    variances.append(variance[:,mask]) \n",
    "    gts.append(gt[:,mask]) \n",
    "means = np.concatenate(means, axis=1)\n",
    "variances = np.concatenate(variances, axis=1)\n",
    "gts = np.concatenate(gts, axis=1)\n",
    "var_uce = UCE(variances, means, gts, 10)[0]\n",
    "var_ence = ENCE(variances, means, gts, 10)[0]\n",
    "var_uce_p = UCE_p(variances, means, gts, 10)[0]\n",
    "var_ence_p = ENCE_p(variances, means, gts, 10)[0]\n",
    "for i, var_name in enumerate(VARIABLES):\n",
    "    print(f\"{var_name}: UCE={var_uce[i]:.5f}, UCE_p={var_uce_p[i]:.5f}, ENCE={var_ence[i]:.5f}, ENCE_p={var_ence_p[i]:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. STDs Coefficient of variation ($C_v$) [Levi 2019]\n",
    "\n",
    "STDs measures the dispersion of the predicted uncertainties. As ENCE and UCE can be one in some trivial cases (i.e. the variance is always the same and matches the error, then both are zero but the uncertainties are not useful), $C_v$ approximates the usefulness of the estimates, it should be maximized, $C_v(\\uparrow)$.\n",
    "\n",
    "As proposed by [Levi 2019], let $\\mu_\\sigma=\\frac{1}{N}\\sum_{i=1}^{N}\\hat\\sigma_i$ be the average standard deviation. Thens $C_v$ is defined as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    C_v = \\frac{1}{\\mu_\\sigma}\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(\\hat\\sigma_i-\\mu_\\sigma)^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn((\u001b[39m10\u001b[39m, \u001b[39m25\u001b[39m))\n\u001b[1;32m      2\u001b[0m x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.random.randn((10, 25))\n",
    "x[0].sum(1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sharpness [Kuleshov 2018, Upadhyay 2023]\n",
    "\n",
    "Inverse idea than $C_v$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0),\n",
       " (1.0, 2.0),\n",
       " (2.0, 3.0),\n",
       " (3.0, 4.0),\n",
       " (4.0, 5.0),\n",
       " (5.0, 6.0),\n",
       " (6.0, 7.0),\n",
       " (7.0, 8.0),\n",
       " (8.0, 9.0),\n",
       " (9.0, 10.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(np.linspace(0, 10, 11))\n",
    "I = [(a,b) for a,b in zip(x[:-1],x[1:])]\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0),\n",
       " (1.0, 2.0),\n",
       " (2.0, 3.0),\n",
       " (3.0, 4.0),\n",
       " (4.0, 5.0),\n",
       " (5.0, 6.0),\n",
       " (6.0, 7.0),\n",
       " (7.0, 8.0),\n",
       " (8.0, 9.0),\n",
       " (9.0, 10.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn = 0\n",
    "mx = 10\n",
    "K = 10\n",
    "[((mx-mn)*i/K, (mx-mn)*(i+1)/K) for i in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85b5837dbff4766d9191b34df77a27f59ea2878bea160d10262c984d96904546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
